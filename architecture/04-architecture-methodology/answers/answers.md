# 第四章 架构方法论 - 习题答案

## 一、选择题

### 1. 答案：C
团队只有3个人就不应该拆微服务。微服务带来的运维复杂度远超小团队的承受能力。"微服务很流行"不是拆分的合理理由。

### 2. 答案：B
熔断器的状态转换：关闭（正常放行）→ 失败率超过阈值 → 打开（快速失败）→ 超时后 → 半开（探测）→ 探测成功回到关闭，探测失败回到打开。

### 3. 答案：C
令牌桶算法允许一定程度的突发流量。桶中可以积累令牌，当突发请求到来时，可以一次性消耗多个令牌。漏桶算法的流出速率恒定，无法应对突发流量。

### 4. 答案：C
增加缓存过期时间不能解决缓存穿透问题。缓存穿透是查询不存在的数据，这些数据在缓存中本来就没有，增加过期时间没有意义。缓存空值和布隆过滤器才是有效方案。

### 5. 答案：B
Kafka中一个Partition只能被Consumer Group中的一个消费者消费。3个Partition最多只能被3个消费者消费，多出的2个消费者将处于空闲状态。

### 6. 答案：B
金丝雀发布先将少量流量（如5%）导向新版本，逐步增加比例，可以在每个阶段观察指标。蓝绿发布是准备好新环境后一次性切换全部流量。金丝雀发布的风险更可控。

---

## 二、简答题

### 7. 缓存击穿 vs 缓存雪崩

**缓存击穿：**
- 定义：某个热点key过期的瞬间，大量并发请求同时打到数据库
- 特点：单个key的问题
- 解决方案：
  1. 互斥锁：缓存未命中时加锁，只允许一个线程查数据库并回填缓存
  2. 热点key永不过期：不设置TTL，通过后台线程定期更新
  3. 逻辑过期：缓存中存储逻辑过期时间，过期后异步更新，当前返回旧数据

**缓存雪崩：**
- 定义：大量缓存key同时过期，或缓存服务整体不可用，导致请求全部打到数据库
- 特点：大面积的问题
- 解决方案：
  1. 过期时间加随机值：`ttl = base_ttl + random(0, 300)`，避免同时过期
  2. 缓存集群高可用：Redis Sentinel或Redis Cluster
  3. 多级缓存：本地缓存 → 分布式缓存 → 数据库
  4. 限流降级：数据库前加限流保护

### 8. 同步通信 vs 异步通信

| 特性 | 同步通信（HTTP/gRPC） | 异步通信（消息队列） |
|------|---------------------|-------------------|
| 响应方式 | 立即返回结果 | 不等待结果 |
| 耦合度 | 较高（调用方需知道被调用方） | 低（通过消息解耦） |
| 可靠性 | 被调用方不可用则失败 | 消息持久化，被调用方恢复后可继续处理 |
| 性能 | 受被调用方响应时间影响 | 调用方不阻塞，吞吐量高 |
| 复杂度 | 简单直观 | 需要处理消息丢失、重复、顺序等问题 |

**同步适用场景：** 需要立即获得结果的操作（查询用户信息、验证密码）、调用链短的场景。

**异步适用场景：** 不需要立即获得结果的操作（发送通知、日志记录）、需要削峰的场景（秒杀）、需要广播给多个消费者的场景。

### 9. Redis Sentinel故障转移

**流程：**
1. 每个Sentinel定期向Master发送PING命令
2. 如果某个Sentinel在指定时间内未收到Master的有效回复，该Sentinel将Master标记为"主观下线"（SDOWN）
3. 当足够多的Sentinel（配置的quorum数量）都认为Master主观下线时，Master被标记为"客观下线"（ODOWN）
4. Sentinel之间通过Raft算法选举一个Leader Sentinel
5. Leader Sentinel从所有Slave中选择最优的一个（优先级最高、复制偏移量最大、运行ID最小）
6. Leader Sentinel向选中的Slave发送 `SLAVEOF NO ONE`，将其提升为新Master
7. Leader Sentinel通知其他Slave复制新Master
8. Leader Sentinel通知客户端新Master的地址

**为什么需要多个Sentinel：** 单个Sentinel可能因为网络问题误判Master下线。多个Sentinel通过投票机制（quorum）确认Master确实不可用，避免误判导致不必要的故障转移。同时多个Sentinel也避免了Sentinel本身的单点故障。

### 10. 重试风暴

**重试风暴（Retry Storm）** 是指在分布式系统中，当下游服务出现故障时，上游服务的重试机制导致请求量成倍放大，进一步加剧下游服务的压力，形成恶性循环。

```
示例：A → B → C，每层重试3次
正常：A发1次 → B发1次 → C收到1次
C故障时：A发1次 → B重试3次 → C收到3次
B也超时：A重试3次 → B每次重试3次 → C收到9次
最坏情况：C收到 3 × 3 = 9 次请求（指数放大）
```

**避免方法：**
1. **限制重试次数：** 每层最多重试2-3次
2. **指数退避+抖动：** 重试间隔指数增长并加随机偏移，避免同时重试
3. **熔断器：** 当失败率超过阈值时停止重试，快速失败
4. **重试预算：** 限制重试请求占总请求的比例（如最多10%的请求是重试）
5. **只在最外层重试：** 内部服务间调用不重试，只在入口层重试

---

## 三、设计题

### 11. 分布式限流系统设计

**整体架构：**
```
API网关 → 限流模块 → Redis集群（存储计数器）
                   → 配置中心（限流规则）
```

**限流规则模型：**
```json
{
  "rules": [
    {"dimension": "user", "key": "user_123", "limit": 100, "window": 1},
    {"dimension": "ip", "key": "192.168.1.1", "limit": 50, "window": 1},
    {"dimension": "api", "key": "/api/v1/orders", "limit": 1000, "window": 1}
  ]
}
```

**关键实现——滑动窗口限流（Redis Sorted Set）：**
```python
def is_allowed(redis, key, limit, window_seconds):
    now = time.time()
    window_start = now - window_seconds

    pipe = redis.pipeline()
    # 1. 删除窗口外的记录
    pipe.zremrangebyscore(key, 0, window_start)
    # 2. 统计窗口内的请求数
    pipe.zcard(key)
    # 3. 添加当前请求
    pipe.zadd(key, {str(uuid.uuid4()): now})
    # 4. 设置过期时间（防止key永不过期）
    pipe.expire(key, window_seconds + 1)

    results = pipe.execute()
    current_count = results[1]
    return current_count < limit
```

**多维度限流：** 对每个请求依次检查用户维度、IP维度、接口维度的限流规则，任一维度超限则拒绝。

**分布式准确性：** 使用Redis的原子操作（Pipeline/Lua脚本）保证计数的准确性。Redis集群保证高可用。

### 12. 消息队列选型

**场景A：日志收集（100亿条/天）→ Kafka**
- 理由：Kafka的吞吐量是百万级/s，适合海量数据场景。顺序写磁盘性能极高，支持数据压缩。日志场景对延迟不敏感，对吞吐量要求高。Kafka与大数据生态（Spark、Flink）集成良好。

**场景B：订单系统（事务消息）→ RocketMQ**
- 理由：RocketMQ原生支持事务消息，可以保证本地事务和消息发送的原子性。适合电商、金融等对数据一致性要求高的场景。同时支持延迟消息（如30分钟未支付自动取消订单）。

**场景C：即时通讯（毫秒级延迟）→ RabbitMQ**
- 理由：RabbitMQ的延迟是微秒级，是三者中最低的。支持灵活的路由规则（Direct、Topic、Fanout），适合消息路由复杂的IM场景。AMQP协议成熟可靠。如果对延迟要求极致，也可以考虑直接使用Redis的Pub/Sub。
